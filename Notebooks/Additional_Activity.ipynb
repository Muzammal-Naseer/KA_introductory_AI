{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCpQaqwoH2Pk"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBuuF9MJKjoe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Download the required libraries (needed when running outside colab where the environment doesn't come pre-loaded with libraries)\n",
        "\n",
        "%pip install torch\n",
        "%pip install torchvision\n",
        "%pip install matplotlib\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtV7omCIKq0K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms.functional import to_tensor, to_pil_image, resize\n",
        "from torchvision import transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBNjc2JpLGIa"
      },
      "source": [
        "# HARD Exercise (optional):\n",
        "\n",
        "### We (Great AI Company X) Welcome New employee. We just fireed a careless and lazy man, but not because he is lazy nor carless. He was fired because he was supposed to create a NN model for MNIST dataset. He destroyed the code by copying from Generative models and the internet without trying to understand anything. He did not want to learn. He just tried to get money without finishing his duties. Unfortunately, we don't have time to create a model from scratch, so you are required to fix the bugs in this notebook.\n",
        "\n",
        "<br/>\n",
        "\n",
        "### We expect that the architecture is destroyed, in addition to many errors of run time, logical, and missing lines. If you fixed this notebook without refering to anything, you will be upgraded.\n",
        "\n",
        "### Notes:\n",
        "1. <B>You are PROHIBITED from using generative models.<B/>\n",
        "1. <B>You are NOT allowed to change the number of layers in the netwrok.<B/> (change only the number of inputs and outputs)\n",
        "\n",
        "# Show us your best, AI detective and developer. ðŸ˜‰\n",
        "\n",
        "## We will use a dataset called MNIST. MNIST contains 60,000 images for training and 10,000 for testing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw5z1ZSoLlh1"
      },
      "source": [
        "## Downloading the dataset\n",
        "\n",
        "### Run the following cells to download the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlJwrK5ZLDeV",
        "outputId": "a2c6bfc0-c177-4346-b38e-37e08c984dd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9912422/9912422 [00:00<00:00, 101853322.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28881/28881 [00:00<00:00, 22633724.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1648877/1648877 [00:00<00:00, 31499154.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [00:00<00:00, 6729257.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_dataset = MNIST(root='./datasets', train=True, download=True, transform=transforms.ToTensor())\n",
        "val_dataset = MNIST(root='./datasets', train=False, download=True, transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmA6buxlh9XI"
      },
      "source": [
        "#### Notice the size of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60ugrf0DMF2N",
        "outputId": "6578ddcd-68fd-4710-f0a3-21a127d3217a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of train_dataset is 60000\n",
            "Length of val_dataset is 10000\n"
          ]
        }
      ],
      "source": [
        "print('Length of train_dataset is', len(train_dataset))\n",
        "print('Length of val_dataset is'  , len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj4XUGKBQmwH"
      },
      "outputs": [],
      "source": [
        "batch_size = 80 / 100\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bw5HNZeQ7Gc"
      },
      "source": [
        "## Let's visualize an image and its channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXxCJ5pQQ5jw"
      },
      "outputs": [],
      "source": [
        "random_img_idx = 0\n",
        "\n",
        "image = train_dataset[random_img_idx][0]\n",
        "label = train_dataset[random_img_idx][1]\n",
        "\n",
        "print(\"The image shape:\", image.shape)\n",
        "print('Number of channels in image: ', image.shape[0])\n",
        "print(\"The image label:\", label)\n",
        "\n",
        "plt.imshow(image.reshape(image.shape[0], image.shape[0]), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsgP23s4QNib"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class MyMNISTClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyMNISTClassifier, self).__init__()\n",
        "\n",
        "    self.linear1 = nn.Linear(28, 512)\n",
        "    self.linear2 = nn.Linear(512, 128)\n",
        "    self.linear3 = nn.Linear(120, 64)\n",
        "    self.linear4 = nn.Linear(64, 9)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = torch.relu(self.linear1(x))\n",
        "    x = torch.relu(self.linear2(x))\n",
        "    x = torch.relu(self.linear3(x))\n",
        "    x = self.linear4(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg_ZO5lNZ5B-"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = MyMNISTClassifier().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5wBeaz7Zdar",
        "outputId": "75c85754-b6ea-435d-cc70-2a873a2a1919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 3\n",
        "lr = 1e-3\n",
        "\n",
        "train_losses = {}\n",
        "\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss(optimizer)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print(f'Using device {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVZ6QIbIZb0A"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for epoch_no in range(num_epochs):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "\n",
        "\n",
        "  for batch_X, batch_y in train_loader:\n",
        "\n",
        "\n",
        "\n",
        "    batch_X = batch_X.view(-1, 748 * 2)\n",
        "\n",
        "\n",
        "    batch_X = batch_X.to(\"cpu\")\n",
        "    batch_y = batch_y.to(\"cuda\")\n",
        "\n",
        "\n",
        "    batch_y_probs = model(batch_X)\n",
        "\n",
        "\n",
        "    loss = criterion(batch_y_probs - batch_y)\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "\n",
        "\n",
        "    epoch_weighted_loss -= len(batch_y) * loss.item()\n",
        "\n",
        "  epoch_loss = epoch_weighted_loss/len(train_loader.dataset)\n",
        "\n",
        "  train_losses.append(epoch_loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  correctly_labelled = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    val_epoch_weighted_loss = 0\n",
        "\n",
        "    for val_batch_X, val_batch_y in train_loader:\n",
        "\n",
        "        val_batch_X = val_batch_X.view(-1, 28 * 28)\n",
        "\n",
        "        val_batch_X = val_batch_X.to(device)\n",
        "        val_batch_y = val_batch_y.to(device)\n",
        "\n",
        "\n",
        "        val_batch_y_probs = model(val_batch_X)\n",
        "\n",
        "        loss = criterion(val_batch_y_probs, val_batch_y)\n",
        "\n",
        "        val_epoch_weighted_loss += len(val_batch_y) * loss.item()\n",
        "\n",
        "\n",
        "        val_batch_y_pred = val_batch_y_probs.argmax(dim=1)\n",
        "\n",
        "        correctly_labelled += (val_batch_y_pred == val_batch_y).sum().item()\n",
        "\n",
        "  val_epoch_loss = val_epoch_weighted_loss/len(val_loader.dataset)\n",
        "  val_losses.append(val_epoch_loss)\n",
        "\n",
        "  print(f'Epoch: {epoch_no}, train_loss={epoch_loss}, val_loss={val_epoch_loss}. labelled {correctly_labelled}/{len(val_loader.dataset)} correctly ({correctly_labelled/len(val_loader.dataset)*100}% accuracy)')\n",
        "\n",
        "print(f'Training complete on device {device}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cMN5vKga0Fk"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses  , label='Val Loss')\n",
        "\n",
        "plt.ylabel('Loss (CCE)')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WSw_onTwF2d"
      },
      "source": [
        "## Play with the model to get a better genralized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LlxO_WOeLRv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
